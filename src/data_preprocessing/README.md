## Processing Data Module

This module prepares the data for input into ensemble machine learning models and neural network models. The following functions are carried out by the moduls, with the corresponding sub-modules mentioned:

1. Data assembly (data_assembler.py) - Combines the various data sources together from the data folder (present in the drive), and provides a unified data to the rest of the modules and sub-modules.

2. Train-test splits (data_features_part1.py) - Carries out a temporal train-test split, which ensures that the test dataset is in the future, to check that the model is robust to future datasets in production.

3. Undersampling (data_features_part1.py) - Due to a severe imbalance in the training data, it undersamples the training data such that for each target=1 in the data, there is an equivalent target= 0 in the data from the same day and data source. If sufficient target= 0 is not available, a lookback and lookforward is used to enusre that there are a minimum number of target= 0 available.

4. Conversion of numbers to strings (data_features_part1.py) - It converts numbers to strings.

5. General-purpose text feature extraction (data_features_part1.py)- The following features are generated by the module:
	1. ```avg_word_length``` - Find the average length of a word in a string.
	2. Sentiment analysis - 
		1. VADER - find sentiments using VADER, which is a lexicon-based sentiment analysis extracted using social media.
		2. AFINN - find sentiment using AFINN, with an integer between -5 (negative) and +5 (positive).
		3. TextBlob - Sentiment of the article using TextBlob. Range is [-1.00, 1.00] with -1.00 implying negative and 1.00 implying positive sentiment.
	3. ```heading_length``` - Length of the title
	4. ```article_length``` - Length of the article
	5. ```num_questions``` - Number of questions marks in the article
	6. ```num exclamations``` - Number of exclamation marks in the article
	7. ```article_sentiment``` - Sentiment of the article using TextBlob. Range is [-1.00, 1.00] with -1.00 implying negative and 1.00 implying positive sentiment.
	8. ```article_subjectivity``` - Subjectivity of the article using TextBlob. The subjectivity is a float within the range [0.0, 1.0] where 0.0 is very objective and 1.0 is very subjective.
	9. ```heading_sentiment``` - Sentiment of the heading
	10. ```heading_subjectivity``` - Subjectivity of the heading
	11. ```ENT_PERSON, ENT_NORP, ENT_ORG, ENT_LOCATION, ENT_PRODUCT, ENT_LANGUAGE, ENT_OTHERS``` - Number of different named entities in the article. Refer to the EDA section for the description of the entities.
	12. ```POS_ADJ, POS_ADV, POS_PROPN, POS_NUM, POS_AUX``` - Number of different parts-of-speech tags in the article. Refer to the EDA section for the description of the POS tags.

6. Data-specific text feature extraction (data_features_part2.py) - 
	1. Count Vectorizer
		1. Character-based - Used a ngram range of (1, 2), removed english stopwords and only used top 1000 frequent features.
		2. Word-based - Used a ngram range of (1, 3), removed english stopwords and only used top 1000 frequent features.
	2. TF-IDF Vectorizer
		1. Character-based - Used a ngram range of (1, 2), removed english stopwords and only used top 1000 frequent features.
		2. Word-based -  Used a ngram range of (1, 3), removed english stopwords and only used top 1000 frequent features.

## Execution Instructions

```
data_preprocessing
│   README.md
│
│
│   data_assembler.py    
│	data_features_part1.py
│	data_features_part2.py
│
│	(helper_functions)
│	helper.py
│	create_features_helper.py
│	module_selction_helper.py
```

data_assembler.py > data_features_part1.py > data_features_part2.py

#### data_assembler.py
It takes data from the ```\data\collected\``` folder and combines all of them together to a unified data format.

#### data_features_part1.py
It builds a dataframe with the general purpose features.

#### data_features_part2.py
It builds a dataframe with the text-specific features.





